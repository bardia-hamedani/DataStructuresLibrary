using System;

namespace Data_Structure_And_Algorithms
{
    public class Multithreaded_Algorithms
    {
        //public enum Operating System Static Thread Properties { These are threads in whichprogrammer should schedule coordinate and manges them so that for even simpl aaplicationsload balancing of threads needs use of complex communication protocols,which will be difficult and error-prone };
        //public enum Operating System Dynamic Thread Properties { These threads use concurrency platformsand are a class of them which are layer of software for mangaing ,coordinating and scheduling prallel processing,so that some of them are as runtime libraries and other as prallel languags with compiler and runtime and programmers dont have inteference with it and its work will be simplified and he only needs to specify the logical parallelism within the computation.these threds has two features:nested parallellism which is spawing a subroutine so that the caller and the callee perform concurently its specified by keyword spawn before a procedure call,althoughthe spqwned thread may be realconcurrrent with its caller ,rather its has logical paralleslismindicaing parts of computaions that may execute in parallel,the the schedule will determine the computations sequnce,,also to use safely values of spawned threads the keyword synch is used which indicates that thread must wait until all spawed children complete their computation before executing next instruction after sych,also bedised this explcit synchronization all procedures execute an impleict sychn before their return to wait for termination of all executing children and parallel loops which are similar to ordinary loops exceptloop iterations can execute concurrently,although its to these loops can be specified using spawn and synch keywords but for convenince its used idreclty,the key word concisists of parallel word preceding for keyword of loops,the compiler implements it through use of nested paralleslism as a divide and conquer subroutine,so that the procedure uses all variables specified in the loop besides 1,and n that are boundaries of the loops as input paramertes then uses i and i` instead of 1 and n in procedure parameter names then uses a statemnt ifi==i` and specifes after it bedoy of loop after them uses esle mid=floor((i+i`)/2) and then spawn recursively the procedure with the same parameters ecept last two are i and mid andafterit call the same procedurewithoutspanw again with the same parameeters  except mid+1 and i` instead of last two parametes,so this procedure recursively spawns first half of loops iteratoin and then calls second half of iterations so that they execute in paralell and then usessynchkeyword,so a binary tree of procdure instances is obtained so that its leaves are single loop iterations,>>>>in this threads their implemnttions is obtained by adding three cuncurency keyword,and by deleting them to achieve its serial version ,>>>>the multithread computation can be represended as a computation dag,>>>>its assumed that our computer classfication is SIMD SM ,which its shared memory is sequentially consiitent that is we have an ideal parllel computer, assuming at each cycle an instrcution from one of threads(processors) is executed that is some global linear order preserving the partial order of the computation dag,so that to understand the the behaviour of any execution.in addition to this asuumbtion,it make the assumbtion that each processor has equal computing power and the scheduling cost is ignorable although algorithms with sufucent parallelism has minimal cost.,>>>>to anlyze running time of multithreaded algorithms two metrics are emloyed work which is total time to execute whole computation on a single processor,that is time taken by all strands and as each strand to take one time unit them its equal to number of strands,span is longest time to execute strnads through dag`s paths,and for with each strand taking onr time unit,its equal to number of vetices in dag`s critical path that is its longest path which is computable in O(V+E) time.besides span and work number of processors and how the scheduler allocates threads will have influence,so that by using subscript number of used processor is indicated so work is running time on a single processor that is T1 and span is running time as each strand runs on its own processors that is number of processors be infiity thatis  Tinfinity,also we candeclare that in one step if a computer has P processors ,it can perform atmost P work  and at Tp time can do at most PTp work and as work tobe done is T1 so PTp>=T1 and so the work law is specified as follows:Tp>=T1/P,also a computer with ulimited number of procesors willbe faster than one with P procssors as it can use number p of its processor to do p processor one job so the span law is as follows:Tp>=Tinfinity also we can see upper boundof speed up is P since Tp>=T1/P and so T1/Tp<=P.Also the ratio T1/Tinfinity the work and span is reffered to as multithreaded computaion parallelism,it indciates average amount of work done in parallel by each step along the critical path,also be maximum possible speedup on any number of procesors,also we have that as the number of processors exceeds  paralellsim computaion wont obtain perfect speedup that is as P>T1/Tinfinity then T1/Tp<=T1/Tinfinity<P.also the parallel slackness of a multithread computation on a ideal parallel computer  with P procesors is defined as (T1/Tinfinity)/P=T1/(PTinfinity) that indicates howmouch the computaion parallellsim exceed number of procesors,so as slackness be less than 1 the perfect speedup is acheived with much less probabilitysince as T1/(PTinfinity)<1 then T1/Tp<=T1/Tinfinity<P and uif slackness exceeds 1 then to acheive perfect speedup work per processor must be condered as a limiting factorwhich can be acheived by a proper scheduler.as nested paralleslim is applied to gain an analysis of the work is the same as analysing runing time of the serilaized algorithm,yet to analyse the algorithms span we hvar that as two subcomputaion are indicated in serial their spans are added together otherwise if they are applied in parallel the maximum of their spans is specified and as paralle loops are employed work is obtained by computing runin time of its serialized procedure,also recursive spawning willincrease work by this increase wont be asymptoticaly since procedure instance binary tree has one node fewer than its leaves hence internalnodes peform constant work to call procedursand leaves conrespondto each oteration pof loop which willbe also costant so if we amortize recusive spwaiing cost over leaves loops iterations work a constant factor willbe acheived,also in this case some concurrency platfroms use coarsening of leaves to execute several iterations on a single leaf either atomutically or by programmer control so to reduce recursive spawning cost although its with expense of increasing parallellsim but if algorithm has enough parallel slackness the near perfect sppedupwont be altered,also to compute span of the algorithm the recursive spanning has eefectssuch that as depth of procedure call tree is logarithm of number of executions of loop that is n and with assumbtion of iteration i having spaniterinfninity(i) then span will be Tinifnity(n)=Thetha(lgn)+max 1<=i<=n iter infiinty(i).alsoas experinces shows work and span will better repreasent perfromance of multithreades algorithms than runinng time,for instance chess multithreaded program *Socrates was bult to be use on a 512 processor computer but has a ptototype on 32 procesor computer but then an optimization for that program wwas presented on 32 processor computer becnhmark having runing times T32=40 instead of T32=65,but with analysisi of work and spanit was cleared that that new program hadslower runing time on 512 prcessor computer,since the original solution has work T1=2048 and span Tinfinity=1 second,if we use equality Tp=T1/P +Tinfinity to compute running time T32 will be65,the optimized version has work T`1=1024 and span T`infnity=8 so T`32=40,but T512=5 and T`512=10 which is twice more slower,because span 8 of new algorithm as wasnt the dominent term in first algorithm become so in scond to increased the runing time ,>>>>the perfomnace ofmultithreaded algorithms also depends on the scheduler,actually cuncurrency platform schedlues synamic threads on static threads and then OS schedules them among procesors but this exrra level for defiing scheduling is unecessary and it can be assumed that concurrency platform schuldes dynamic threads to processors directly.scheduler must schedule threads online that is with no knowledge aboutwhen they spawn or be complete,also it should operate in distributed manner that is threads implemeting scheduler perfomrso that the computaion gain load baalacing,so an online centralized scheduler is considered which is aware of global computaion step at any time,also we assume that its greedy that is it assignes as many strands as possible to processors at each time,also we assume if at cycle P processors be available then its called a complete step and greedy scheduler assignes whole of them to processors,and if fewer threads exists then the step is called incomplete step and the greedy scheduler assignes the threads to their own processors.work law indicates the best ruuning time is Tp=T1/P and span law indicates its Tp=Tinfinity,then as an advantage greegy schedulers use sum of them as an upperbound,that is, its possible to indcate as a theorem that in a ideal parallel computer with P processors a greedy scheduler executes a multithreaded computaion with work T1 and span Tinfinity in TP<=(T1/P)+Tinfiinty,also we canhvae following corollary taht for with ideal p procesor computer with a greedy scheuleris equal a less than factor 2 of optimal that is with T&p be runing itme of optimalshceduler we have Tp<=2T*p,alsoasanother corollay we can see that with a greedy scheduler as slackness grows ,near-perfect speed up is acheved that is if P<<T1/Tinfinty we have Tp~T1/P that is speedup equivalently is equal to P,which as a rule of thumb the slackness of at least 10 can indicate << notation ,which then spantermin the main theorem will be leass than 10% of work per procesors whichis ideal in many engineering situations. ,>>>>the multithreaded rprograms also can be deterministic or nondetrministic,bit therfe are cases in which because of determinacy races a determinitic programs can be executed determinsitically.the deterinacy race occurs a two computations acess same memory locaton and at least one of them is going to peforma write operation ,this conditoin cuase is that every operation is perrfomed through some instrcution cycle which can be specified as 1.rading a memory location to one of the proccesor registers,2.perfrom the operation,3.write the resulted value back to memory,which parallel execution of threads can be considered as interleaving of instruction with respect to computation dag duo to sequential consistency of ideal prallel computers,the race condition occurs as follows when a procssors reads a value from mmeory toits register  and modifies it then another proccesor does the same the that same memory location and put its modified in its registerand writes that value in the previous memory location,and then the first processorwrites its registervaluei nthemeory so that result todiffer from execution of serialized form of this proceudre,but it can be seen that as one of computaton had executed before othr instead of simulatenously the race wont be seenalso if they were simultnaouse there will be many ordering that race wontoccur,this is the problemistic aspect of dterminacy races that many ordering have proper results but some dont. so testing algorithm for them will be extermely difiicult.although its possible to synchronizethread using mutualexculsion locks and other methods but we asume that parallel executing strands are independent  and dont have determinacy races so parallel loops iterations are independent and between spwn and sycnh keywords childs thread and additional bored ones` codes are independent of parent code ,also argumnets of apwned threads are evaluted before spawiing it and so this evalution is performed togehter with parent thread instrcutions.salso algorithms with determinacy race may perform corrency as teo threads write the same calue in same location to result be accurate.famous race bugs wereTherac-25 radiation theraphy machine which killed three people and injured several and North American Blackout of 2003 which left over 50 milltion peolpe without power,};
        //public enum Fib Algorithm Properties { fibincci numbers are computd as folows---- and folloeing code----,but this computation does much repeated work so producing a depp recursive procedure instences tree and as memoization wont be used many reapetd call to the procedure with the same argument is odne,>>>>toanalyze its runing time we have that since this procedure make two recurisve cals plus a constant ork we haev T(n)=T(n-1)+T(n-2)+Tetha(1) we solution T(n)=Tetha(phi^n) which phi=(1+5^(0.5))/2 which is the golden ratiosince the runiing time grows expenntially with n ,algorithm will be a aslow one. };
        //public enum P-Fib Algorithm Properties { its modification of Fib algorithm in which calls toFib(n-1) and Fib(n-2) be independts they can be excuted in an y order and they computaion not toaffect eah other adn so to callthem in paraelel as folows:---,>>>>to anlalyze it we have that to compute work of it since serizaletion of this algorithm will result Fib procedure so wrok wil be Fib`s running time which is T1(n)=T(n)=Tetha(phi^n).to compute the span as spawned call to Fib(n-1) and call to Fib(n-4)are paralel,so the span will be Tingity=max(Tinfinty(n-1),Tinfinty(n-2))+Tetha(1)=Tinfinty(n-1)+Tetha(1) whichits solution will be Tinfinty(n)=Thetha(n) and hence paralelism will be T1(n)/Tinfinty(n)=Tetha((phi^n)/n) which growsignificantlywith n andhence having high alackness so that even modest values of n will acheive near perfect speedups };
        //public enum Operating System Dynamic Thread Computation Dag Properties { its thr directed acyclic graph G=(V,E) representing multithread computation in which its vertices are instructions and th edges (v,u) indicates that insrtcution v mus execute before intrcution u.,>>>> if a chain of instrcutions which doenst involve parallel control that are spawn,explcit and implicit synch they are refered as strands,and edges be correponding prallel control instrcutins.also as two strands have a directed edge between them then they are refferd to be (logically) inseries and otherwise to be(logically) in parallel.also its possible to consider the computation dag tobe embedded in the tree of procedure instances and its possible to ssurround strandsbelonign to same  procedures by a rectangle.also the edges represent dependeciesbetween strands so that a continuation edge (u,v`) connects a strand to its sccessor in same procedure instance and isdrawn horizonally,the spawn edge (u,v) which is downward and connects strand u tostrand v as u spawns v,the call edge (u,v) is a downward edge representing a normal procdur call,also the differncebetween spawn and call edge is that in spawning strand u,the spqwinig procedure creates a continous edgetostrand u` in its procedreindicating that u` ca execute xonxurrenltly with strand v,and the return edge (u,x) which is employed as a procedure returns to its calling procedure with x be the strand immediatly after next synch in the calling procedure and is upward.also comptaions begins with a single initial strand and ends in single final strand, };
        //public enum Mat-Vec Algorithm Properties {this algorithm indciates multipication of an n*n matrix A=(aij)to an n-vector x=(xj) in paraael through thispseodocode:----,also the parallel loops canbe implemented through this divide and conquar subroutine:---,>>>>to analyze it we have thatto compute its work its serialized algoeithm`s runing time is computed which is T1(n)=Tetha(n^2),to compute the span the first loop has span theta(lgn) since recursive spwiing has more order then each loop iteration and for the second loop since each iteration span is Tetha(n) duo to the serial loop,and as other parts of algorithm has constnt span ,algorithm span will be Tetha(n),and henc with parallelsim Tetha(n^2)/Tetha(n)=Tetha(n)};
        //public enum P-Square Matrix Multiply Algorithm Properties { this algorithm is obtained by parallelizing loops of algorithm Multiply,>>>>to analyze it we have that as this is obtained by serializing Multiply work will be its runinng itme that is T1(n)=Tetha(n^3),to compute the span we have that since the path along the firt and second parallel loops are travesed and then a seauantial loop n time is iterated so Thetha(lgn)+Thetha(lgn)+Thetha(n)=Thetha(n) and hence parallelsismwill be Thetha(n^3)/Thetha(n)=Thetha(n^2) }; 
        //public enum P-Matrix Multiply Recursive Algorithm Properties { this procedure multiplies two matrices A and B toC by partioning A,B and C to four n/2* n/2 submatrices so eight multiplication of n/2 * n/2 matirces plusone addition of  n/2 * n/2 matirces is done to using a devide and conquere appraoch,and nested parllelsim is employed also to avoid unnecessary matrix allocation outut mtrix is specifieid as parameter for recursion,line 3 perfrom multiplicatinof base case in whcih 1*1 matrices are muliplied,and in lines 4-17 re3cursion is performed so that inline 4 a tempoarary matrix is defined in line 5 matrices A,B,C and T are partioned to  n/2 * n/2 matirces.lie6 put submatric porduct A11B11 to C11.C12,C21 and C22 are computed in lines 7-9 which are first term in final redult addition,also in line 10 T11 which is equal to A12B21and T12, T21 and T22 are computed in the same fashion in ines11-13 which are second term in final result asston,first seven recursive call are spawned and last is in main strand and inline 14 a synch is papecified and then through two paralle loops T and C are aaded togehtr,>>>>to analyze it we have that to obtain work the srilzed algortihm is analyzed in ehich each partionening eill take Tetha(n) time  and ew have eight recurive calls of n/2*n/2matrices and then a Tetha(n^2) iteration so we have following recurene M1(n)=8M1(n/2)+Tetha(n^2)=Tetha(n^3) which is same as triply nesteed loops of P-Square Matrix Multiply procedure.to compute the span of algorithm we hvae that span of partioning is Tetha(1),soubly nested loop have span Tetha(lgn) and since all eigh recurive calls use matrices of the same size as arammetrs all have the smae tree depthso we have follwing recurence Minfinity(n)=M1(n/2)+Tetha(lgn )which will be  Minfinity(n)= Tehtha((lgn)^2)so parallelsim will be Thetha(n)/Thetha(n)=Thetha((n^3)/((lgn)^2)};
        //public enum P-Strassen Multiply Algorithm Properties { this algorithm is similar to Strassen_Multiply except nested parallelsim is used,1.divideing A,B,C to n/2*n/2 matrices  which takes Tetha(1) time,2.ceater 10 n/2*n/2 matrices S1,..,S10 whcih are addition or subtraction of matrices obtained in last step which takes Tetha(n^2) work and Tetha(lgn) span by use of soubly nested paralel loops.3.by applying matrices created in two last steps ,computation of seven n/2*n/2 product matrices P1,...,P7 tobe spawened recursively,4.computing C11,C12,C21,and C22 by adding and subtracting proper Pi matrices by using parallel loops,which can be computed with Tetha(n^2) work and Tetha(lgn) span,>>>>toanlayze it we have that since serilized algotihm is the same as original algorithm runing itme of serial cverion is work of this algotihm which is Tetha(n^(lg7)),then to compute span since seven recurives with eaual size parameters exists the same recurrrence as P-Matrix Multiply Recursive algorithm is obtained which has solution of Tetha((lgn)^2),soparallelsim of this algoithm will be Thetha((n^(lg7))/((lgn)^2) which is high although be slighltly lass than P-Matrix Multiply Recursive algorithm. };
    }
}
