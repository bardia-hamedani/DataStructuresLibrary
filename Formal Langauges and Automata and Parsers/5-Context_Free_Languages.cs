using System;

namespace Formal_Languages_And_Automata
{
    public class Context_Free_Languages
    {
        //public enum Parsing Properties {explaining a sentence through its grammatical derivation,Meaning that to ask how we can tell if a given string is derivable from a given grammar it’s a way of describing sentence structure , >>>> its important when we need to understand the meaning of a sentence as we do e.g. in translating from one language to other and is interpreters, compilers and other translating programs ,>>>>finding a sequence of productions by which a w elemnt of L is derived};
        //public enum Context Free Languages Properties { although regular languages are useful there are also nonregular languages also some properties of programming languages require something beyond regular languagesInorder to cover this and other more complicated features we must enlarge the family of languages this leads us to consider context-free languages and grammars,a language is context-free iff there is a context free grammar G such that L=L (G),>>>> it’s the most important aspect in computation theory since programming languages has many features that can be described by them and these features has important applications in design of programming languages as well as compilers,>>>>it has a central position in hierarchy of formal languages since they include important but restricted language families such as regular and deterministic context-free languages and also there are broader language families so that context-free languages are special case of them so to to realize the relationship between language families and to classify specific languages in appropriate categories. And their similarities and differences we should investigate characteristic properties of the various families by expressing closure under a variety of operations, algorithms for determining properties of members of the family and structural results such as pumping lemmas};
        //public enum Context Free Grammer Properties {to create grammars that are more powerful we relax some of restrictions in regular grammars so that  its left side must be a single variable and anything in right is permitted,>>>> all productions in P have the form A => x where A an element of V and x an element of V union T*,a grammar G= (V,T,S,P) is context-free grammar if all productions in P have form A ->x where A is inV and x is in (V Union T) ,>>>>it derive their name from the fact that the substitution of the variable on the left of a production can be made any time such a variable appears in a sentential form it doesnt depend on the symbols in the rest of the sentential form,this feature is the consequence of allowing only a single variable on left side of productions ,>>>> in context- free grammars that are not linear a derivation may involve sentential forms with more than one variable in such cases we have a choice in the order in which variables are replaced in these orders derivations not only yield the same sentence but also use exactly the same production the difference is only in the order in which the productions are applied so we consider two derivations to remove such irrelevant factors.Regular and linear grammars are clearly context-free , but a context- free grammar is not necessarily linear.,>>>>the family of regular languages is a proper subset of context free languages};
        //public enum Leftmost String Derivation Properties { a derivtion is said to be left mostif in each step the leftmost variable in the sentential form is replaced };
        //public enum Rightmost String Derivation Properties { a derivtion is said to be right most if in each step the rightmost variable in the sentential form is replaced ,>>>>they are sometimes called canonical derivations};
        //public enum Left Sentential Form Properties { if S =* lm>a then we say that a is a left sentential form of the grammar at hand };
        //public enum Derivation Parse Tree Properties { Its a way of showing derivations independent of order in which productionsare used is a parse tree is a graphical representation of a derivation that filters out the order in whichproductions are applied to replace nonterminals each interior node of a parse tree represents the application of a production,meaning that it shows which productions are used butdonot give the order of their application or reflecting its irreleveance,>>>> a node labled with a variable occuring on the left side of a production has children consisting of the symbols on the right side of that production,so beginning with root labeled with start symbol and ending with leafs which are terminals a derivation tree shows how each variable is replaced in the derivation the following definition makes this notion precise: let G=(V,T,S,P) be a context free grammar an ordered tree is a derivation tree for G iff it has the following properties:1)the root is labeled S,2)every leaf has a label from T union lambda,3) every interior vertex that isnt leaf has a label from V,4) if a vertex has label A an element Of V and its chilren are labled a1 ,..., an its grammer has A -> a0a1an production,5) a leaf labeled lambda has no siblings that is a vertex with a child labled lambda can have no other children,>>>> a tree that has properties 3 4 and 5 but in which property 1 does not hold and in which property 2 is replaced by (2a) every leaf has a label from V union T union lambda is said partial derivation tree,>>>> to see the relationship between derivations and parse trees any derivation a1 ... an for each derivation ai we can construct parse tree whose yeild is ai as follows suppose we already have constructed a parse tree with yield a(i-1)=X1 ... Xk suppose ai is derived from a(i - 1) by replacing Xj a nonterminal by b = Y1 ... Ym to model this step of the derivation find the jth leaf from the left in the current parse tree this leaf is labeled Xj, give this leaf m children labled Y1 ... Ym from the left  as a special case if m equals to 0 then b equals to epsilon and we give the jth leaf one child labled epsilon,>>>>since a parse tree ignores variations in the order in which symbols in sentential forms are replaced there is a many yo one relationship between derivations and parse trees,the string of symbol obtained by reading the leaves of the tree from left to right in the order they are encountered when the tree is travered in a depth first manner always taking leftmost unexplored branch omitting any lambdas encountered,since derivation trees give an explicit description of derivation and are great help in making arguments we establish the connection between derivations and derivation tree by this theorem: Let G=(V, T, S, P) be a context-free grammar than for every w in L(G) there exists a derivation tree of G whose yield is W conversely the yield of any derivation tree is in L(G) also if tG is any partial derivation tree for G whose root is labeled S then the yield of tG is a sentential form of G to prove it first show that for every sentential form of L(G) there is a corresponding partial derivation tree we do this by induction on number of steps in derivation,the claimed result is true for derivations in one step since S-> u implies S -> u then assume for every  sentential form derivable in n steps there is a corresponding partial derivation tree now any w derivable in n+1 steps is such that S -> x ay, x,y is in(V Union T) , A is in V in n steps and xAy -*> xa1a2… amy = w, ai is in V is in T since a partial derivation tree by inductive assumption has yeild xAy and we have A -> a1a2…an by expending leaf labled A we get partial derivation  xa1a2…amy=w so by induction we claim that result is true for all sentential forms in a similar way we can show that every partial derivation tree represents some sentential form and also since a derivation tree is also a partial derivation tree whose leafs are terminals it follows that every sentence in L(G) is the yield of some derivation tree of G and that gield of every derivation tree is in L (G) ,>>>> since any W L(G) has a derivation and then we can always get a left most derivation if we think that tree is built so that the left most variable in the tree was always expanded first so filling in a few details we conclude that any W L(G) has a left most and night most derivation. };
        //public enum Exhaustive search Parsing Properties { we systematically construct all possible derivations and see whether any of them match w, at round one we start by looking at S -> x productions finding all x derivable from S in one step if match havnt found we go to next round in which we apply all applicable productions to the leftmost variable of every x,so we will have a set of sentential forms some of them possibly leadly 60 w 50 on each round we take all left most variables and apply all possible productions. some of these sentential forms can rejected on the grounds that w can never be derived from them but in general we will have on each round a set of possible sentential forms,Generative aspects of grammars consists of set of strings that can be derived using it but in practical applications we also concerned with the analytical side of grammar which are member ship algorithm and parsing and we have that ,so in first round productions are gained by one derivation step in second round with two steps and so on and if w is in L(G) it must have a left derivation of finite length,>>> it’s a form of top-down parsing.,>>>>The most obvious we can generalize this idea and make a theorem for it theorem: If G=(V,T,S,P) is a context-free grammar which doesn't have any rule of form A ->  or A ->B where A,B are in V then the exhaustive search parsing method can be made into an algorithm which for any W in 4 either produces a parsing of W or tells us that no pursing is possible to prove it we have that if we consider both length if a each sentential form and its number of terminal symbols we see that at each step of derivation at least one of them increases since neither length of a sentential form nor number of terminal symbols can exceed |w| a derivation can't lauda more than 2|w| round) at each time we aither have a successful parsing or not.,>>>> one of it flaws is its rediousness it isnt to be used where efficient parsing is required,>>>> its another flaw is that its possible that it never terminates for strings not in L,to overcome nontermination of exhaistive search parsing we must restrict the form that gramer can have by removing A -> lambda and A -> B productions,>>>>given any w in {a,b} exhaistive search parsing method will always terminate in no more than w rounds because the length of the sentential form grows by at least one symbol in each round,>>>>after w rounds ,we either pared w or we see that w isnt in L, although exhaustive search parsing guarantees that parsing can be done but number of sentential dorms generated by it is large, we can find how many sentential form is generated since it depends on that case but we can find an upper bound for it if we consider only left most derivations we can have no more than |P| sentential forms in round one, |P| sentential form in round two and so on and since parsing can't involve more than 2|W| rounds so total number of sentential forms cont exceed M= |P|+|P|^2+…+|P|^(2|w|)=O(P^(2|w|+1)) so work for exhaustive search parsing may grow exponentially with length of string so cost will be prohibitive although equation is only a bound and actual number of sentential forms are smaller nevertheless practical  ,>>>>for every context free grammer there existes an algorithm that parses any w in L in a number of steps proportional to w power 3};
        //public enum Simple Grammar Properties { since a parsing method with time complexity of |w| is still quite in efficient so we want a linear time parsing in which it takes time proportional to the length of the string we don't know any linear time parsing methods for content- free languages but such algorithms can be found for simple grammars we have that a context-free grammar G= (V,T,S,P) is a simple grammar or s-grammar if all its productions are of the form A ->ax where A is in V , a is in T, x is in V and any pair (A,a) occurs at most once in P,>>>> although these grammars are quite restrictive they are of some interest since many features of programming languages is described by it,>>>>if G as an s grammar then any string w in L can be parsed with an efford to |w|,to prove it we have that if we use exhaustive search parsing using W= a1 ac…an since there can he at most one rule with S on left starting with a1 at right derivation begin with S -> a1A1A2…Am next we substitute A1 and again since there is at most one choice we have S -> a1a2B1B2…A2…so each step produces one terminal symbol & so whole process most be completed in no more than |w| steps. .>>>>its used in grammar definitions of programming languages as keywords then terminal on left of right side of production wil be keyword  so keywords not only provide some visual structure to guide the reader of a program but also make the work of compiler easier so that, that statement is easily and efficiently parsed but since some features of a programming language cant be expressed by s-grammars we use LL and LR grammars};
        //public enum Ambiguous Context Grammar Properties {a context free grammar is ambigous if there exists some w in L that has at least two distinct derivation or a grammar that produces more than one parse tree for some sentence, alternatively ambiguity implies the existance of two or more leftmost or rightmost derivations so a number of differnt derivation trees may exist,,also we have that this ambiguity comes from the grammar meaning that it can b removed by frudney an equivalent unambiguous grammar,>>>>ambiguity is a common feature of natural languages but in programming we remove it ambiguity could be removed by finding an equivalent unambiguous grammar by rewriting it with an idea for its productions, ambiguity can be removed by expressing a general rule for it this disambiguating rule can theoretically be incoporated directly into a grammar but in practice its rarely built into the productions,One disambiguating rule is that is as done in programming languages to associate procedure rule for operators but this resolution is outside the grammar & its better to number the grammar also we can have a disambiguating rule so that to introduce new variables & replacing the productions, but its not too hard to solve it in specific instance but in general questions like if a given context-free grammar is ambiguous or whether two given context-free grammars are equivalent are very difficult to answer ,>>>> its a fact that every ambiguous grammar fails to be LR and thus is not in any grammr classes however its certain types are useful for language constructs like expressions an ambiguous grammar provides a shorter more natural specification than any equivalent unambiguous grammar another use of ambiguous grammars is in isolating commonly occuring syntactic constructs for special case optimization so we can specify the special case constructs by carefully adding new productions to the grammar eg we can easily change the associativity and precedence of operators without disturbing the productions or the number of states in the resulting parser second the parser for the unambiguous grammar wil spend a subtantial fraction of its time reducing productions whose funcion is to enforce associativity and precedence the parser for ambiguous grammar will not waste time reducing by these single productions although we use ambiguous grammars in all cases we specify disambiguating rules allowing only one parse tree for each sentence in this way the overall language specification becomes unambiguous and sometimes it becomes possible to design an LR parser follwing the same ambiguity resolving choices but ambiguous constructs should be used sparingly and in controlled fashion there can be no guarantee as to what language is recognized by a parser..............,>>>>the issue of ambiguity takes added significance when dealing with programming languages so taht the specification of a programming language must be unambiguous otherwise the program may yeild difernt results on different compilers or different systems so we must be able to  recognize and remove ambiguities  also we should reconnize that whether a language is or isnt inherently ambiguousso we need algortihms for detecting and remiving ambiguities in context free grammars and for deciding whether or not a context free language is inherently ambiguous but these are difficult and impossible in most general sense };
        //public enum Unambiguous Context Free Grammar Properties {if L is a context free language for which there exists an unambiguous grammar then L is unambiguous,>>>>if G=(V,T,S,P) be a contetc free grammar in which every A in V occurs on the left side of at most one prosuction then G is unambuguous};
        //public enum Inherently Ambiguous Language Properties { if L is a context free language , if every grammar that generates L is ambiguous then language is called inherently ambiguous,>>>>its difficult even to exhibit an inherently ambigous language the we can do is give an axample with some reasonably plausible claim that its inherently ambigous e.g. when we can decompose grammar of a languauge to two distinct grammars and then showing that these two gramars have conflicting requirements for example putting restrictions on number of two varaibles {a^nbn} and {a^nb^m} to to conclude that its impossible to combine these requrements in a single set of rules that cover e.g. the case n=m uniquely };
        //public enum Backus_Naur_Form_BNF_Grammar_Properties { its tradtional when considering programming languages to use convention for specifying grammars called the Backus_Naur form or BNF its like usual garmmars but appearance is different in BNF variables are enclosed in triangular brackets terminal symbols are written BNF also uses subsidiary symbols such as | in the same way we have done ,=: is used instaed of -> also BNF uses more explicit varaible identifier  tom amke the intent of the production explicit };
    }
}
