using System;


namespace Data_Structure_And_Algorithms
{
    public class Growth_Of_Functions
    {
        //public enum Algorithm Asymptotic Running Time Properties { when we look at input sizes large enough to make only the order of growth of running time relevant we are studying the asymptotic efficiency of algorithms we observe how the running time of an algorithm increases with the size of the input in the limit as the size of the input incresaes without bound usually an algorithm asymptotically more efficint will be the best choice for all but very small inputs also determining exact step count of a program can prove to be an exceedingly difficult task expending immense efford to dtermine step count excatly is not a vary worthwhhile endeavour since notion of a step is itself inaxact in some situations for example when the difference in stepcounts of two programs is very large we might feel safe in predicting that bigger euqation runs faser but even in this case we dont need exact step something like its 80n is adequate to arrive same conclusion we need only statements like c1n2 and c2n2 then eg second equation faster for sufficiently large nso there is n so that beyond it second is faster its called break even point the exact value of break even point cant be determined analytaclliy the programs have to run on a computer in order to determine the break event pointto know that there is e break even point its enough teo knnow two equationsso we need some terminology to make meaningful statements about time and space complexities of a program and also asymptotic complexity can be determined quite easily without determining the exact step countthis is usually done by first determining the asymptotic complexity of each statement or group of statements in program then adding up these complexities so its like computing step count with table but know after computing multiplication of frequency and step per execution we compute in asymptotic running time and we can assign asymptotic complexity of a program by taking a global approach without need of computing step count its so that we assume for loops iteration times then for recursive calls we assume eg for input n it iterates n - 1 times and for in asymptotic analysis of while loops we may see division be 2 in each iteration in instance characteristic so its iterated Tetha(logn) times in worst case sometimes i while loops we must estimate number of its executions in the loop from the program itself we see that we can say that (6(n^3))/(logn+1) = O(n^3) also eg sum of two powered terms eg 33*(n^3) + 4*(n^2) is both Omega(n^2) and Omega(n^3) but not that (n^2)*logn = tetha(n2) and (n^2)/logn = theta(n2) also if we have tow functions eg facorial and log in ech other asymptotic of smmaler one is sed,asymptotic notations can be used within mathematical fomulaseg equasl means membership if we have eg n equsal O n2 in general however when asymptotic notation appears in a formula we interpret it as standing for some anonymous function that we do onot care to nameeg if tetyha n be in right side of ewaulation then it meanns f(n) where f(n) is some function in the set tetha(n) this way can help elimnate enessential detail andclutter in an equation also in formulas number of anonymous functions in an expression is understood to be equal to the number of times the asymptoic notation appearseg Zigma O(i) has only a single anonymous function function of inot n functions in some cases asymptotic nation appears on the lft hand side of an equation we interpret such equations using following rule no matter how the anonymous functions are chosen on the left of the equal sign there is a way to choose the anonymous functions on the right of the equal sign to make the equation valid in othe words the right side of an eqaution provides a coarser level of detail than left hand side this rule can be used for a number of relationships in chain so we say for any function in left eg g(n) = tetha(n) thereis some function for which h(n)= tetha(n^2),>>>>many of relational properies of real numbers apply to asymtotic comparisons it has transivty f(n)= tetha(g(n)) and g(n)=tetha(h(n)) imply f(n)=tetha(h(n)) it has reflexivity f(n) = tetha(f(n)) it has symmetry f(n) = tetha(g(n)) if and only if g(n)=tetha(f(n)) it has transpose symmetry f(n)= O(g(n)) if and only if g(n)=Omega(f(n)) also we can draw an analogy between the asymptotic comparison of two functions f and g and real numbers a and b f(n) = O(g(n)) is a <= b f(n) = Omega(g(n)) is >= b f(n) = Tetha(g(n)) is a = b f(n) = o(g(n)) is a < b f(n) = omega(g(n)) is a > b we say f(n) is asymptotically smaller than g(n) if f(n) = o(g(n)) and f(n) is asymptotically larger than g(n) if f(n) = w(g(n)) but one property of real numbers however does not carry over to asymptotic notation trichotomy for any two real numbers a and b exactly one of the following must hold a < b a = b or a > b so not all functions are asymptotically comparable so f and g may be we have neither f(n) = O g(n) nor f(n)=Omega(g(n)) og function n ^( 1 + sinx),>>>>in comparing asymptotic notations eg imaggine P has comlpexity n Q has Taetha(n^2) so P is faster Q eg P is bounded above by cn and Q is bounded from below by dn2 then since cn <= dn2 for n >= c/d P is faster Q whenevr n >= max{n1,n2,c / d} we must aware of the presence of the phrase sufficiently large when deciding which of two programs to use we must know whether the n we are dealing with is in fact sufficiently largeeg if program P actually runs in 10^6 n miliseconds whereas program Q runs in n ^ 2 milliseconds and if we have n smaller than other factors being equasl program Q is fasetr,for asymptotic notations we have that log n < n < nlohn < n2 < n3 < 2 ^ n but we see utility of programs with exponential complexity is limited to samll ntypically n <= 40 programs having a complexity that is a polynomial of high degree are also of limited utilityso from apractical standpoint its evident for reasonably large n say n > 100 only programs of small comlpexity such as n,nlogn,n^2,n^3 are feasiblefurther this is teh case even if one could build a computer capable of executing 10^12 instuctions per second };
        //public enum Algorithm Tetha Asymptotic Notation Properties { for a given function g(n)we denote by tatha(g(n)) the set of functions tetha(g(n)) ={ f(n): such that there exist positive constants c1,c2 and n0 such that 0 <= c1*g(n)<=f(n) <= c2*g(n) for all n >= n0} so that f(n) belongs to tatha(g(n)) if there exist positive constants c1 and c2 such that it can be sandwiched between c1g(n) and c2g(n) for sufficiently large n because tatah(g(n)) is a set we write f(n) elemnt of tetha(g(n)) we can also use equasl intead of it functions are such that for all values of n to right of n0 value of f(n) lies at or above c1*g(n) and at or below c2*g(n) in other words for all n>=n0 the function f(n)= g(n) to within constant factor we say that g(n) is an asymptotic tight bound for f(n) the defnition of tetha(g(n)) requires every member of tetha be asymptoticallt nonnegative for large n g(n) also should be so otherwise Tetha(g(n) is empty,>>>>when we want to show that a function belongs to this set we put eg c1*(n^2) and c2*(n^2) in two direction of the function then we find corresponding values for n and c to fit in equationimportant thig isone choice exista different function belonging to tetha(n^2) would usually require different constantsto show that a function doesnt exst in this set we use contradiction then we obtain something that eg n < c2/6 which isnt true for arbitrary large n since c2 is constant so if we can the correctness of membership of a function in this set so we set c1 to a value slightly amaller than the coefficient ofthe highestorder term and set c2 to a value slightly larger permits the inequalities in definition of tatha notation to be satiified for the function itself,>>>>for any two functions f(n) and g(n) we have f(n)= tatha(g(n)) if and only if f(n) = O(g(n)) and f(n) = Omega(g(n)) we it for obtaing upper and lower bound from tight bound we can do inverse and obtain tight bound from upper and lower bound,we have that max{f(n),g(n)}= Tetha(f(n) + g(n)) also (n + a) ^ b = Tetha(n^b),>>>>running time of an algorithm is Tetha(g(n)) if and only if its worst case running time is O(g(n)) and its best case running time is Omega(g(n)),>>>>in asymptotic notations coefficients in all of the g(n) is used in the all asymptotic nataions have been 1 };
        //public enum Algorithm O Asymptotic Notation Properties { when we have only an symptotic upperbound we use O notation for a given function g n we denote by O g n pronounced big oh of g of n the set of functions O g n equals f n there exist positive constants c and n0 such that 0 smaller than and equal f n smaller than and equal c g n for all n bigger than and equal n0 in diagram for all values n to the right of n0 the value of the function f n is on or below g n O logn algorithm id faster for sufficiently large n then O n similarly O nlogn is btterthan O n2 but not as good as O n,statement fn equasl O g n sattes only that g n is an upper bound on the value of f n for all n bigger equasl n0 it doesnt say anything about how good this bound is eg n is O n2 n is O n power 2 point 5 for sattement f n is O g n to be informative g n shouldbe as small a function of n as one can come up with for which f n eqausl O g n ,from definition of O it should be clear that fn equasl O g n is not the same as O g n equasl f n in fact its meaningless to say that O g n equals f n,if f n equasl amnpowermplus to a1n plus a0 then f n O n power m,f n is tatha g n implies f n O g n written set theoretically we have tatha g n subset of O g n,using O notation we can often describe the runig time of an algorithm merely by inspecting the algorithms overall structueeg a douply nested loop has a O n power 2 upper bound on the wort case runing time,since O notation describes an upper bound when we use it to bound the worst case running time of an algorithm we have a bound on the running time of algorithm on every inputbut tetha bound on the worst case running time of an algorithm does not imply that bound on running time of that algorithm on every inputeg when input be sorted it differs eg is tetha n,although we say that ruuning time of an algorithm is eg O n2 but for agiven n the Actaual runing time varies depending on particular input sizeso we really mean that there is a function f n that is eg O n2 such that for any value of n no matter what particular input of size n is chosen running time on that input is bounded from above byvalue f n ,we can extend our notation to the case of two parameters n and m that can go to infinity independently at different rates for a given function g n m we denote O g n m set of functions O g n m equals f n m such that there exist posiive constants c n0 and m0 such that 0 smaller tjan and eqiual f n m smaller than anda equal c g n m for all n bigger than and equal n0 and m bigger than and equalm0,if p n bbe a polynomial with degree d k be a constant then we have if k bigger and equal d then p n equals O n power k if k smaller and equal d then p n equals Omega n power k if k equal d then p n equals Tetha n power k if k bigger d then p n equals o n power k if k smaller d then p n equals omega n power k};
        //public enum Algorithm Omega Asymptotic Notation Properties { it providesan symptotic lower bound for a given function g n we denote by Omega g n pronounced big omega of g n the set of functions Omega g n equals f n such that there exist positive constants c and n0 such that 0 smaller than equal c g n smaller than and equal f n for all n bigger than and equal n0eg in chart for all values n to the right of n0 the value of f n is on or above c g n,there are several functions g n for which f n equasl Omega g n the function g n is only a lower bound on f n for the saattement f n Omega g n to be informative g n should be as large a function of n as possible for which the stetment f n Omega g n is true,if f n equasl amnpowerm plus to a1n plus a0and am bigger 0then fn equasl Omega npower m,since Omega notation describes a lower bound when we use it to bound the best case runnning time of an algorithm by implication we also bound the runnig time of the algorithm on arbitrary inputs as well eg best case ruunning time of an algorithm be Tetha n its runing time is also Tetha n eg worst case running time of an algorithm may not be Tatha n2 since there should be an input for which that algorithm runs in tetha n biut we can say worst case running time of that algorithm is Omega n2 since there may be an input forwhcih algorithm takes Tetha n2but when we say the running time no modifier of an algorithm is Omega g n we mean no matter what particular input size n is chosen for each value of n running time is atleasta constant time g n for sufficiently large n,if fnequasl am n power m plus to a1n plus 10 and am bigger 0 then f n equasl Tetha n power m};
        //public enum Algorithm o Asymptotic Notation Properties { we use this notation to denote an upper bound not asymptotically tightwe formally define o g n littel oh of g of n as the set o g n f n such that for any positive constant c bigger than 0 there exists a constant n0 bigger than 0 such that 0smaller than and equal f n smaller than c g n for all c bigger than n0eg 2n equasl o n2 but 2 n2 not eqaul o n2 so differnce betwen O and o is that in O bound is for some constant c bigger han 0but in o is for all c bigger than 0,in the o notation the function fn become insignificant relative to g n as n approaches infinitytaht is lime n to infinity f n devide g n is 0 };
        //public enum Algorithm omega Asymptotic Notation Properties { we use it to denote a lower bound not asymptotically tight we define omega g n littel omega of g of n the set omega g n equals fn such that for any positive canstant c bigger than 0 there exists a constant n0 bigger than such that 0 smaller than equal cgn smaller f n for all n biogger than ad eqaual n0eg n2 devide 2 equasl omega n but n2 devide 2 isnt equal omega n2so we have lime n to infinity f n devide gn is infinity f n bcomes arbitrary large relative to g n as n approaches infinity };



    }
}
