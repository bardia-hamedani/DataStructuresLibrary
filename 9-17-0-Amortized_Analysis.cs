using System;


namespace Data_Structure_And_Algorithms
{
    public class Table<T>
    {
        public double size,num;
        public Stack<T> Stack;
        public Table(int TableSize)
        {
            size = TableSize;
            Stack = new Stack<T>(TableSize);
        }
    }
    public class Amortized_Analysis<T>
    {
        //public enum Amortized Analysis Properits { in that the time required to perform a sequence of data structure operations is averaged over all the operations performed it can be used to show that the average cost of an operation is small if one averages over a sequnce of operations even though a single operation within the sequence might be expensive,>>>>it differs from average case analysis in that probability is not involved an amortized analysis guarantees the average performance of each operation in the worst case,>>>>three common techniques used in amortized analysis are aggregate analysis accounting method and potential method,>>>>the insight into a particular data structure gained by performing an amortized analysis can help in optimizing the design };
        //public enum Amortized Analysis Aggregate Analysis Technique Properits { in that we show that for all n a sequence of n operations takes worse case time T(n) in total in the worse case the average cost or amortized cost per operation is therefore T(n)/n,>>>>note that this amortized cost applies to each operation even when there are several types of operations in the sequence };
        //public enum Amortized Analysis Accounting Method Technique Properits { in accounting method of amortized analysis we assign differing charges to different operations with some operations charged more or less than they actually cost the amount we charge an operation is called its amortized cost when an operations amortized cost exceeds its actual cost the differences is assigned to specific objects in the data structure as credit and it can be used later on to help pay for operations whose amortized cost is less than actual cost thus one can view the amortized cost of an operation as being split between its actual and credit that is either deposited or used up this method is very different from aggregate analysis in which all operations have the same amortized cost,>>>>if we want ti use to to hsow the worst-case of average cost per operations the total amortized cost of a sequence of operations must be an upper bound on the total actual cost of the sequence and this relationship must hold for all sequences of opeations so if we denote the actual cost ofthe ith operation by ci and the amortized cost of the ith operation by cbari we require Zigma i=1...n cbari >= Zigma i=1 ... n ci for al sequences of n operations the total credit stored in the data structure is the difference between the total amortized cost and the total actual cost or (Zigma i=1 ... n (cbari)) - (Zigma i=1...n (ci)) total credit is always nonnegative };
        //public enum Amortized Analysis Potential Method Technique Properties { instead of representing prepaid work as stored credit we can represent it as potential energy o just potential that can be released to pay for future operations and its associated with data structure as a whole rather than specific object of data structure,>>>>it worksas follows we start with an initial dta structure D0 on which n operations are peformed for each i=1,2,...,n we let ci be the actual cost of the ith operation and Di be the structure that results after applying the ith operation to data structure D(i-1) a potential functon P map each data structure Di to a real number P(Di) which is the potential associated with data structure Di the amortized cost cibar of the ith operation with respect to potential functoin P is defined by cbari= ci + P(Di)- P(D(i - 1)) the amortized cost of each operation is its actual cost plus the increase in potential due to the operation so total amortized cost of n operatons is Zigma i=1 ... n cbari = Zigma (i = 1... n (ci)) + P(Dn) - P(D0) if P(Dn)>=P(D0) then totalamortized cost Zigma i=1..n cbari is upper bound on total actual cost Zigma i=0..n ci andsince we dont know how many operation may be perfomred if we need that P(Di)>= P(D0) then we  guarantee that we pay in advance but its often conventient todefine P(D0)to be 0 and then P(Di)>=0 for all i,>>>> if the potential difference P(Di)- P(D(i-1)) of the ith operation is positive then the amortized cost cbari represents an over chrage to the ith operation and the potential of the data structure increases,>>>> if the potential difference P(Di)- P(D(i-1)) of the ith operation is negative then the amortized cost cbari represents an under chrage to the ith operation and the potential is paid by the decrease in the potential,>>>>amortized costs depend on the choice of potential functions may yeild different amortized costs yet still be upper bounds on the actual costs and there are often trade offs that can be made in choosing a potential function best choice depends on desired time bounds };
        //public enum Increment_Algorithm_Properties { it implements a k-bit binary counter conting upward from 0we use an aray A[0..k-1] of bits where length[A]=k a binary number x stored in counter has its lowest-order bit in A[0] and highest in A[k-1] so x=Zigmai=0..k-1 A[i]*2^iinitially x=0 and so A[i]=0 for i=0,1,...,k-1 then we add 1(modulo 2^k) to value of counter at teh start of each iteration of while loop in lines 2-4 we add a 1 to position i if A[i]=1 then adding 1 make that bit 0 and yeilds a carry of 1 to be added into position i+1 on next iteration of loop otherwise loop ends and then if i<k we know that A[i]=0 so adding a 1 into position i hnages 0 to 1 its i line 6,>>>>to analyze its runing time we have that cost of each Increment is linear in number of bits flipped,>>>>we can use aggregate analysis for it so that a cursoy analysis for sequence of n operations yeilds a bound that is corect but not tight a single execution of Increment takes time Theta(k) in worst-case in array with all 1 so a sequence of n Increment operations on an initially zero counter takes time O(nk) in worst case we can tightenour analysis to obtain a worst-case of O(n) for a sequence of n Increment by observing that not all bits flip each time Inceremnt is called A[0] flips each time Increment is called next-higher-odrer A[1] flips only every other time  so a sequqence of n Increemnt operations on a initially zero counter causes A[1] to flip ceiling[n/2] timesA[4] to be every four time and ceiling[n/4] times in general for i=0,1,...,ceiling[lgn] bit A[i] flips ceiling[n/2^i]times for i>ceiling[lgn] bit A[i] never flips at all so total number of flips in the sequence isa Zigma i=0..[lgn] [n/2^i]<n*Zigma i=0..infinity (1/2^i)=2n so worse-case time for a sequence of n Increment operations on an initially zero counter is O(n) and average amortized cost per operation is O(n)/n=O(1),>>>>to use the accounting method of amorized analysis we use a dollar bill torepresent each unit of cost andlet us charge an amortized cost of 2 dollars to set a bit to 1 when a bit is set we use 1 dollar(out of 2 dollars charged )to pay for setting a bit and we place other dolar on bit as credit to be used later when flip the bit back to 0 so we neednt charge anything to reset a bit to 0 so to determine amortized cost of Increment at most one bit is set in line 6 so amortized cost of an Increment operation is at most 2 dollars and 1`s number will be never negative so for n Increment operations total amortized cost is O(n) bounding total actual cost,>>>>to use the potential method of amorized analysis so we define potential of the counter after ith Increemnt operation to be bi the number of 1`s in the counter afetr ith operation so to compute amortized cost of it we suppose ith Increment operation resets ti bits its actual cost will be ti+1 since adition to resseting ti bits it sets at most one bit to 1 if bi =0 ith opertion resets al k bitsand so b(i-1)=ti=k if bi>0 then bi=b(i-1)-ti+1 in either case bi<=b(i-1)-ti+1 so P(Di)-P(D(i-1))=1-ti so amortized cost is cbari<=(ti+1)+(1-ti)=2 if counetr starts at zero P(D0)=0 and since P(Di)>=0 for all i totoal amortized cost of sequqence of n Increment oerations is upper bound on total actual cost  and so worst-case cost of n Increment operations is O(n) and if counter does not start at zeroin tha there are initailly b0 1`s and after n Incrementoperations there are bn 1`s  where 0<=b0,bn<=k and since cbari<=2 P(D0)=b0 and P(Dn)=bn so tital actual cost of n Inceremnt operations is Zigma i=1..n (ci)=2n-bn+b0  and b0<=k as long as k=O(n) total actual cost is O(n)  meaning that if we execute n-Omega(k) Increemnt operations total actual cost is O(n) indepenednt of initial value};
        public void Increment(int[] A)//Worse-Case Time Tethe(n)
        {
            int i = 0;
            while (i < A.Length && A[i] == 1)
                A[i++] = 0;
            if (i < A.Length)
                A[i] = 1;
        }
        //public enum Decrement_Algorithm_Properties { in this procedure n operations could cost as much as Theta(nk) time };
        public void Decrement(int[] A)//Worse-Case Time Tethe(n)
        {
            //int[] A = new int[5] { 1, 0, 0, 0, 0 };
            //int[] A1 = new int[5] { 0, 0, 1, 1, 0 };
            //new Amortized_Analysis().Increment(A);
            //new Amortized_Analysis().Decrement(A1);
            int i = 0;
            while (i < A.Length)
            {
                if (A[i] == 1)
                {
                    A[i] = 0;
                    for (int j = i - 1; j >= 0; j--)
                        A[j] = 1;
                    break;
                }
                i++;
            }
        }
        //public enum Dynamic_Table_Data_Stucture_Properties { in some applictions we dont know in advance how many objects will be stored in table we might allocate some apce and later find out that its not enough the table must be reallocated with laregr size and objects stoed in original table must be copied to new larger table let us assume taht storage for a table is allocated  as an array of slotsa table fills up when all slots have been used or equivalently whe its load factor is 1 so since our software nvironmrnt proveds a memory management system thus when an item is inserted into a full table we can expand teh able by allocating a new table with more slots tahn old one and since we need table to reside in contiguous memry  we copy items from the old into new able a common heuristic is to aalocate a new table having twice as many slots as old oneif insertion is done laod factor of table is always at least 1/2 so amount spaced wasted never exceds alf total space ,>>>>we assume synamic table supports operations Table-Insert abd Table-Delete,>>>>details of data-structuring method used to organize table are unimportant we might use a stack a heap or a hash tabel also we might use an array or collection of arrays to implement object storage,>>>>we define the load factor A(T) of a nonpty table T to be number of items stored in the table divided by size (number of slots) of the table we assign an empty table size 0 and we define its load factor to be 1 if load factor of a dynamic table is bounded below by a constant unused space in tabel is never more than a constant fraction of total amount of space };
        //public enum Table_Insert_Algorithm_Properties { it inerts into the table an item taht occupies a single slot taht is a space for one item in this code we assume T is an object representing table field table[T] conatins a pointer to block of storage representing the table field num[T] contains number of items in table and field size[T] is total number of slots in table initially table is empty  num[T]=size[T]=0 here we have two "insertions" procdures:Table-Insert and elemntary insertion into table in lines 6 and 10 we analyze running time of Table-Insert in terms of elementary insertions by assgning cost 1 to elelemntary oneswe assume this relationship is inear in individual insertions sooverhead of alocating an initial table in line 2 is constant and overhead for allocating and freeing storage in lines 5 and 7 is dominated by cost transferring items in line 6 and we call lines 5-9 an expansion,>>>>to analyze it runing time we ahve that if we want to have cursory  we analyze a sequence of n Table-Insert operations on an initailly empty table to obtain cost ci of ith opertion if table ahs room ci=1 since only line 10 is performed if table is full an expansion ocurs then ci=i sinc cost is 1 for elemntary insertion in line 10 plus i-1 for items that must e copied from old tabel to new table in line 6if n operations is perfomed so worst-case cost of anoperation is O(n) leading to an upper bound of O(n^2) but this bound isnt tight since cost of table expasion is not borne often in corse of n Table-Insert operations specifically ith operation causes an expansion only when i-1 is an eact powerof 2 the aortized cost of an operation is in fact O(1) by using aggregate analysis as follows cost of ith operationis ci={ i if i-1 is an exact power of 2,1 otherwise } so total cost of n Table-Insert operationsis Zigma i=1...n (ci)<n+2n=3n since total cost of n Table-Insert operations is 3n the amortized cost os a single operation is 3,>>>>to use accounting method of amortized analysis intuitively each item pays for 3 elemntary insertions:inserting itself in curent table,moving itself when table is exanded,and moving another item taht has already moved once when table is expandede.g. suppose we havea tbale of size mafter expansion then numbr of items in table is m/2 and not canting any creditwe charge 3 dollars for each insertion the leemntary insertion occuring immedaitely cots 1 dollar another dolalr is placed as credit on the item inserted the third dollar is placed as credit on one of the m/2 already in table and filling table requires (m/2)-1 additional insertions an when table contains m items and is full each item has a dollar to pay for its reinsertion during expansion,>>>>to use potential method of amortized analysis we start by defiing a potentail function P that is 0 immediately after an expansion but builds to able size when table is full so taht next expansion can be paid for by potential the function P(T)=2*num[T]-size[T] is one possibility immediately after an expansion we have num[T]=size[T]/2 and thus P(T)=0 as desired immediately before expansion we have num[T]=sizet] and P(T)=num[T] as desied initial value o fpotential is 0 so table is always at least half full num[T]>=size[T]/2 implying taht P(T) is always nonnegative so sum of amortized costs of n operations is an upper bound on sum of actual costto analyze amortized cost of ith Table-Insert operation let numi be number of items stored in table after ith operation sizei be total size of table after ith operation and Pi be potential after ith operation initially we have num0=0 size=0 and P0=0 if ith Table-Insert does not trigger an expansionthen we have sizei=size(i-1) and amortized cost of operation is cbari=3 if ith operation does trigger an expansion then we have sizei=2*size(i-1) and size(i1)=num(i-1)=numi-1 implying sizei=2*(numi-1) so amortized cost of operation is cbari=3};
        public void Table_Insert(Table<T> T, T x)
        {
            //Table T = new Table(10);
            //for (int i = 0; i <= 12; i++)
            //    new Amortized_Analysis().Table_Insert(T, i);
            //for (int i = 0; i <= 10; i++)
            //    new Amortized_Analysis().Table_Delete(T);
            if (T.size == 0)
            {
                T.Stack = new  Stack<T>(1);
                T.size = 1;
            }
            if(T.num==T.size)
            {
                Stack<T> S = new Stack<T>((int)(2 * T.size));
                for (int i = 0; i <= T.Stack.Capacity - 1; i++)
                    S.Push(T.Stack.Pop());
                T.Stack = S;
            T.size = 2 * T.size;
            }
            T.Stack.Push(x);
            T.num++;
        }
        //public enum Table_Delete_Algorithm_Properties { it removes an item from table thereby freeing a slot it is often desirabel to contract table when load factor becomes to small so taht wasted space is not exorbitant it means we allocate a new smaller table and then copy teh items from old table to new one storage of old table can tehn be freed by returngign it to memory-management syatem ideally we would like to presevre two proepritews:(1)the load factor of dyaanmic atble is bounded by a contant and (2)the amortized cost of a table operation is bounded above by a constant a natural strategy for expansion and contraction is double table size when an item is inserted into ful table and halve the size when a the size when a deletion would cause the table to become less than half full this strategy guarantees that load factor never drops below 1/2 but unfortunately it can cause amortized cost of an operation to be quite largee.g. if n/2 insertions take place costing a total of Theta(n) for next n/2 we have this sequence:I,D,D,I,I,D,D,I,I,...  first insertion causes an expansion of table to size n two following deletions cause a contraction of table back to size n/2 two further insertions cause another expansion and so forthcost of each expansion and contraction is Theta(n) and there are Theta(n) of them so total cost of n operations is Theta(n^2) and amortized cost of an operation is Theta(n)difficulty is after an expansion we do not perform enough deletions to pay for a contraction likwise after a conttarction we dont perform enough insertions to pay for an expansion we can improve upon this strategy by allowing load factor ro drop below 1/2 so we constinueto double table size when an item is inserted to a full tablebut we halve table size when a deletion causes the table to become less than 1/4 full rather tahn 1/2 so load factor of table is bounded below by constant 1/4 idea is taht after expansion laod factor is 1/2 so half of items must be deletedbefore a contraction can occur since contarction does ot occur unless load factor fall below 1/4 likewise after a contarction laod factor is also 1/2 s number of items in table must be doubled by insertions before an expansion can occursince expansion occur when lad factor exceed 1 also if number of items in able drops to 0 storage for table is freed taht is num[T]=0 then size[T]=0,>>>>to use potential method of amortized analysis we use a potential fucntion P taht is 0 immediately afetr an expansion or contarction and builds as laodfactor increases to 1 or decrases to 1/4 and load factor of an nonempty atbel T is A(T)=num[T]/size[T] and whether table is empty or not we use this potetntial fucntion P(T)= { 2*num[T]-size[T] if A(T)>=1/2  size[T]/2-num[T] if A(T)<1/2 } ans since ppotential of an empty table is 0 and potential is never negative so amoritzed cost of poerations wiht respect to P is upper ound on actual cost of sequence when A=1/2 P=0 when A=1 then P=num[T] so potential can pay for an expansion if an item is inserted when A=1/4 we haev P(T)=num[T] so potential can pay for contracion if an item is deletd to analyze a sequqence of Table-Insert and Table-Delete we letci be actual cost of ith operation and cbari be amortized cost with respect to P numi be number of items stored in table afetr ith opertaion sizei be total size so table afetr ith operation Ai be laod factor of atble afetr ith operation and Pi be potential afetr ith opertaion initially num0=size=P0=0 and A0=1 so if ith operation is Table-Insert if A(i-1)>=1/2 whetehr atbel expands or not cbari is at most 3 if A(i-1)<1/2 table cant expand as a result of of opertaion since expansio occurs when a(i-1)=1if Ai<1/2 amortized cost of ith operation is cbari=0 if A(i-1)<1/2 but Ai>=1/2 thencbari=3 so aamortized cost of Table-Inset is at most 3 when ith operation is Table-Delete if A(i1)<1/2 we must consider whether operation causes a contarction if it does not tehn cbari =2  if A(i-1)<1/2 and ith operation does trigger a contraction ci=numi+1 since we deete one item and move numi items and cbari=1 when A(i-1)>=1/2 amortized cost is also bounded above by a constant in summery since amortized cost of each operation is bounded above by a cpnstant actual time for any sequence of n operations ois O(n) };
        public T Table_Delete(Table<T> T)
        {
            if (T.size == 0)
                return default(T);
            if (T.num <=(int)Math.Floor((0.25)* T.size))
            {
                Stack<T> S = new Stack<T>((int)Math.Floor(0.5D * T.size));
                for (int i = 0; i <= (int)Math.Floor((0.25) * T.size) - 1; i++)
                    S.Push(T.Stack.Pop());
                T.Stack = S;
                T.size = (0.5D) * T.size;
            }
            T.num--;
            return T.Stack.Pop();
        }
    }
}
